{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data = pd.read_csv(filepath_or_buffer=\"bank-additional-full.csv\", delimiter=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows if these 5 columns contains unknown job\tmarital\teducation\n",
    "# default(has credit in default?)\thousing(has housing loan?)\tloan(has personal loan?)\n",
    "bank_data = bank_data[(bank_data['job'] != 'unknown')\n",
    "                      & (bank_data['marital'] != 'unknown')\n",
    "                      & (bank_data['education'] != 'unknown')\n",
    "                      & (bank_data['default'] != 'unknown')\n",
    "                      & (bank_data['housing'] != 'unknown')\n",
    "                      & (bank_data['loan'] != 'unknown')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'illiterate': 0, 'basic.4y': 1, 'basic.6y': 2, 'basic.9y': 3, 'high.school': 4, 'professional.course': 5, 'university.degree': 6}\n"
     ]
    }
   ],
   "source": [
    "# Ordinal encoding-- education\n",
    "edu_mapping = {label:idx for idx, label in enumerate(['illiterate', 'basic.4y', 'basic.6y', 'basic.9y',\n",
    "    'high.school',  'professional.course', 'university.degree'])}\n",
    "print(edu_mapping)\n",
    "bank_data['education']  = bank_data['education'].map(edu_mapping)\n",
    "\n",
    "# Label encoding pdays\n",
    "bank_data['pdays'] = (bank_data['pdays'] >998).astype(int)\n",
    "\n",
    "\n",
    "# Label encoding y(independent variable)\n",
    "bank_data['y'].replace(('yes', 'no'), (1, 0), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# One hot encoding and filling in missing values if missing values is present\n",
    "cat_si_step = ('si', SimpleImputer(strategy='constant',fill_value='MISSING'))\n",
    "cat_ohe_step = ('ohe', OneHotEncoder(sparse=False,handle_unknown='ignore'))\n",
    "cat_steps = [cat_si_step, cat_ohe_step]\n",
    "cat_pipe = Pipeline(cat_steps)\n",
    "cat_cols = [1,2,4,5,6,7,8,9,14] #removed education\n",
    "cat_transformers = [('cat', cat_pipe, cat_cols)]\n",
    "\n",
    "# remainder should be passthrough so that the numerical columns also included in the result\n",
    "ct = ColumnTransformer(transformers=cat_transformers,remainder='passthrough')\n",
    "\n",
    "X_cat_transformed = ct.fit_transform(bank_data.iloc[:,:])\n",
    "X_cat_transformed.shape\n",
    "\n",
    "pl = ct.named_transformers_['cat']\n",
    "ohe = pl.named_steps['ohe']\n",
    "# showing the columns name after encoding\n",
    "a = ohe.get_feature_names()\n",
    "cat_col_names = a.tolist()\n",
    "\n",
    "\n",
    "ncol_name = cat_col_names + [\"age\",\"education\",\"duration\",\"campaign\",\"pdays\",\"previous\",\"emp.var.rate\",\"cons.price.idx\", \"cons.conf.idx\", \"euribor3m\", \"nr.employed\", \"y\"]\n",
    "bank_data_final = pd.DataFrame(data=X_cat_transformed[:,:],columns=ncol_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns to prevent dummy variable trap\n",
    "# we need to drop 'duration' as suggested in the readme.txt\n",
    "bank_data_final.drop(['x0_unemployed','x1_single','x2_no','x3_no','x4_no','x5_telephone','x6_sep','x7_wed','x8_success'],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering-----------------\n",
    "X = bank_data_final.iloc[:, 0:42].values\n",
    "y = bank_data_final.iloc[:, 42].values\n",
    "\n",
    "# drop duration\n",
    "X = np.delete(X, [33], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Feature Scaling for numerical attributes only\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train[:, np.r_[31, 33:41]] = sc.fit_transform(X_train[:, np.r_[31, 33:41]])\n",
    "X_test[:, np.r_[31, 33:41]] = sc.transform(X_test[:, np.r_[31, 33:41]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number transactions X_train dataset:  (24390, 41)\n",
      "Number transactions y_train dataset:  (24390,)\n",
      "Number transactions X_test dataset:  (6098, 41)\n",
      "Number transactions y_test dataset:  (6098,)\n",
      "Before OverSampling, counts of label '1': 3111\n",
      "Before OverSampling, counts of label '0': 21279 \n",
      "\n",
      "After OverSampling, the shape of train_X: (42558, 41)\n",
      "After OverSampling, the shape of train_y: (42558,) \n",
      "\n",
      "After OverSampling, counts of label '1': 21279\n",
      "After OverSampling, counts of label '0': 21279\n"
     ]
    }
   ],
   "source": [
    "# Solving imbalance output problem(accuracy paradox) by oversampling\n",
    "print(\"Number transactions X_train dataset: \", X_train.shape)\n",
    "print(\"Number transactions y_train dataset: \", y_train.shape)\n",
    "print(\"Number transactions X_test dataset: \", X_test.shape)\n",
    "print(\"Number transactions y_test dataset: \", y_test.shape)\n",
    "\n",
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train == 1)))\n",
    "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train == 0)))\n",
    "\n",
    "sm = SMOTE(random_state=2)\n",
    "X_train, y_train = sm.fit_sample(X_train, y_train.ravel())\n",
    "\n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_train.shape))\n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train.shape))\n",
    "\n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train == 1)))\n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train == 0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying LDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "lda = LDA(n_components = 2)\n",
    "X_train = lda.fit_transform(X_train, y_train)\n",
    "X_test = lda.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build-from-scratch logistic regession code with Joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.01, num_iter=100000, fit_intercept=True, verbose=True):\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "\n",
    "    def __sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def __loss(self, h, y):\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "\n",
    "    def fit_i(self,X,y,i):\n",
    "        z = np.dot(X, self.theta)\n",
    "        h = self.__sigmoid(z)\n",
    "        gradient = np.dot(X.T, (h - y)) / y.size\n",
    "        self.theta -= self.lr * gradient\n",
    "\n",
    "        z = np.dot(X, self.theta)\n",
    "        h = self.__sigmoid(z)\n",
    "        loss = self.__loss(h, y)\n",
    "\n",
    "        if (self.verbose == True and i % 10000 == 0):\n",
    "            print(f'loss: {loss} \\t')\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "\n",
    "        # weights initialization\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        Parallel(n_jobs=3, require='sharedmem')(delayed(self.fit_i)(X, y, i) for i in range(self.num_iter))\n",
    "\n",
    "\n",
    "        #for i in range(self.num_iter):\n",
    "         #   self.fit_i(X,y,i)\n",
    "\n",
    "\n",
    "    def predict_prob(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "\n",
    "        return self.__sigmoid(np.dot(X, self.theta))\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.predict_prob(X).round() #threshold = 0.5\n",
    "\n",
    "\n",
    "    def score(self):\n",
    "        preds = model.predict(X_test)\n",
    "        return (preds == y_test).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.6628634015246153 \t\n",
      "loss: 0.525733808484878 \t\n",
      "loss: 0.525733808484878 \t\n",
      "loss: 0.525733808484878 \t\n",
      "loss: 0.525733808484878 \t\n",
      "loss: 0.525733808484878 \t\n",
      "loss: 0.525733808484878 \t\n",
      "loss: 0.525733808484878 \t\n",
      "loss: 0.525733808484878 \t\n",
      "loss: 0.525733808484878 \t\n",
      "loss: 0.525733808484878 \t\n",
      "loss: 0.525733808484878 \t\n",
      "loss: 0.525733808484878 \t\n",
      "loss: 0.525733808484878 \t\n",
      "loss: 0.525733808484878 \t\n",
      "loss: 0.525733808484878 \t\n",
      "loss: 0.525733808484878 \t\n",
      "loss: 0.525733808484878 \t\n",
      "loss: 0.525733808484878 \t\n",
      "loss: 0.525733808484878 \t\n",
      "loss: 0.525733808484878 \t\n",
      "loss: 0.525733808484878 \t\n",
      "loss: 0.525733808484878 \t\n",
      "loss: 0.525733808484878 \t\n",
      "loss: 0.525733808484878 \t\n",
      "loss: 0.525733808484878 \t\n",
      "loss: 0.525733808484878 \t\n",
      "loss: 0.525733808484878 \t\n",
      "loss: 0.525733808484878 \t\n",
      "loss: 0.525733808484878 \t\n",
      "It costs 212.12741804122925sec\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(lr=0.1, num_iter=300000)\n",
    "tStart = time.time()\n",
    "model.fit(X_train,y_train)\n",
    "tEnd = time.time()\n",
    "print (\"It costs \" + str(tEnd - tStart) + \"sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.7935388652017055\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.81      0.87      5350\n",
      "         1.0       0.33      0.67      0.44       748\n",
      "\n",
      "   micro avg       0.79      0.79      0.79      6098\n",
      "   macro avg       0.64      0.74      0.66      6098\n",
      "weighted avg       0.87      0.79      0.82      6098\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(X_test)\n",
    "print(\"The accuracy is \" + str((preds == y_test).mean()))\n",
    "#%%\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "model.theta\n",
    "\n",
    "#%%\n",
    "# Making classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4336, 1014],\n",
       "       [ 245,  503]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
